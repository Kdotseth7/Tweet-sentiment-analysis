{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from autocorrect import spell\n",
    "from hashtag_separator import infer_spaces\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('..', 'data')\n",
    "OUTPUT_PATH = os.path.join('..', 'output')\n",
    "\n",
    "POS_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'pos_train.txt')\n",
    "NEG_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'neg_train.txt')\n",
    "\n",
    "POS_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'positive-words.txt')\n",
    "NEG_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'negative-words.txt')\n",
    "\n",
    "with open(POS_TRAIN_PATH, 'r') as f:\n",
    "    pos_data = f.read().splitlines()\n",
    "with open(NEG_TRAIN_PATH, 'r') as f:\n",
    "    neg_data = f.read().splitlines()\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "# Credit: https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107\n",
    "with open(POS_WORDS_PATH, 'r') as f:\n",
    "    pos_words = f.read().splitlines()\n",
    "with open(NEG_WORDS_PATH, 'r') as f:\n",
    "    neg_words = f.read().splitlines()\n",
    "    \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    return list(filter(lambda x: x not in stopwords_eng, words))\n",
    "\n",
    "def is_number(word):\n",
    "    original_word = word\n",
    "    special_chars = ['.', ',', '/', '%', '-']\n",
    "    for char in special_chars:\n",
    "        word = word.replace(char, '')\n",
    "    if word.isdigit():\n",
    "        return '<number>'\n",
    "    else:\n",
    "        return original_word\n",
    "    \n",
    "def remove_numbers(words):\n",
    "    return list(map(is_number, words))\n",
    "\n",
    "def replace_hashtags(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word and word[0]!='#':\n",
    "            new_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        new_words.append('<hashtag>')\n",
    "        for hash_word in infer_spaces(word[1:]).split(' '):\n",
    "            new_words.append(hash_word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "# Credit: https://stackoverflow.com/questions/10072744/remove-repeating-characters-from-words\n",
    "def emphasize(word):\n",
    "    if (word[:2]=='..'):\n",
    "        return '...'\n",
    "    \n",
    "    new_word = re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "    if len(new_word) != len(word):\n",
    "        return '<<' + spell(new_word) + '>>'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def emphasize_words(words):\n",
    "    return list(map(emphasize, words))\n",
    "\n",
    "def lemmatize(word):\n",
    "    try:\n",
    "        temp = lemmatizer.lemmatize(word).lower()\n",
    "        return temp\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_words(words):\n",
    "    return list(map(lemmatize, words))\n",
    "\n",
    "def infer_sentiment(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (word[:2]=='<<'):\n",
    "            check_word = word[2:-2]\n",
    "        else:\n",
    "            check_word = word\n",
    "        \n",
    "        if check_word in pos_words:\n",
    "            new_words += ['positive', word]\n",
    "        elif check_word in neg_words:\n",
    "            new_words += ['negative', word]\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "def emphasize_punctiation(words):\n",
    "    special_chars = ['!', '?']\n",
    "    i = 1\n",
    "    \n",
    "    while (i<len(words)):\n",
    "        word1 = words[i-1]\n",
    "        word2 = words[i]\n",
    "        if (word1 in special_chars and word2 in special_chars):\n",
    "            start = i-1\n",
    "            while (i+1<len(words) and words[i+1] in special_chars):\n",
    "                i += 1\n",
    "            words = words[:start] + ['<emphasis>'] + words[i+1:]\n",
    "            i = start\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tweet(t):\n",
    "    words = t.split(' ')\n",
    "    words = remove_stopwords(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = replace_hashtags(words)\n",
    "    words = emphasize_words(words)\n",
    "    words = normalize_words(words)\n",
    "    words = infer_sentiment(words)\n",
    "    words = emphasize_punctiation(words)\n",
    "    tweet = ' '.join(words)\n",
    "    return tweet\n",
    "\n",
    "def parse_data(data):\n",
    "    parsed = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, t in enumerate(data):\n",
    "        if (i+1)%5000==0:\n",
    "            print(str(i+1)+'/'+str(len(data)), time.time()-start_time)\n",
    "        parsed.append(parse_tweet(t))\n",
    "    \n",
    "    print('Total time (s): ' + str(time.time()-start_time))\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 6.638222932815552\n",
      "Total time (s): 6.638969898223877\n",
      "5000/5000 6.084514141082764\n",
      "Total time (s): 6.0858073234558105\n"
     ]
    }
   ],
   "source": [
    "parsed_pos = parse_data(pos_data[0:5000])\n",
    "parsed_neg = parse_data(neg_data[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(parsed_pos)\n",
    "pos_df.columns = ['text']\n",
    "pos_df['label'] = 'pos'\n",
    "\n",
    "neg_df = pd.DataFrame(parsed_neg)\n",
    "neg_df.columns = ['text']\n",
    "neg_df['label'] = 'neg'\n",
    "\n",
    "df = pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "PARSED_TWEETS_PATH = os.path.join(OUTPUT_PATH, 'parsed_tweets.csv')\n",
    "df.to_csv(PARSED_TWEETS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('parsed_test.csv') as f:\n",
    "    cl = NaiveBayesClassifier(f, format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify('I suck dick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./test_data.txt', 'r') as f:\n",
    "    test_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/10000 7.3515729904174805\n",
      "200/10000 14.485529899597168\n",
      "300/10000 22.302922010421753\n",
      "400/10000 29.161931037902832\n",
      "500/10000 36.11663603782654\n",
      "600/10000 43.951600074768066\n",
      "700/10000 51.47758078575134\n",
      "800/10000 58.543980836868286\n",
      "900/10000 65.5224220752716\n",
      "1000/10000 72.61669516563416\n",
      "1100/10000 79.66771697998047\n",
      "1200/10000 86.70493912696838\n",
      "1300/10000 93.74563312530518\n",
      "1400/10000 100.66223812103271\n",
      "1500/10000 107.4223120212555\n",
      "1600/10000 114.47362279891968\n",
      "1700/10000 121.5224449634552\n",
      "1800/10000 128.64934515953064\n",
      "1900/10000 135.6797730922699\n",
      "2000/10000 142.72301506996155\n",
      "2100/10000 150.5981569290161\n",
      "2200/10000 157.54157304763794\n",
      "2300/10000 165.45492315292358\n",
      "2400/10000 174.4098241329193\n",
      "2500/10000 181.92578792572021\n",
      "2600/10000 189.03197503089905\n",
      "2700/10000 197.71762108802795\n",
      "2800/10000 205.74543809890747\n",
      "2900/10000 213.60946488380432\n",
      "3000/10000 224.17110681533813\n",
      "3100/10000 232.73848605155945\n",
      "3200/10000 240.20346093177795\n",
      "3300/10000 248.08762311935425\n",
      "3400/10000 255.29530882835388\n",
      "3500/10000 262.9164471626282\n",
      "3600/10000 270.2203211784363\n",
      "3700/10000 277.95413422584534\n",
      "3800/10000 285.8543498516083\n",
      "3900/10000 293.28763818740845\n",
      "4000/10000 300.92553901672363\n",
      "4100/10000 308.37322998046875\n",
      "4200/10000 315.7904210090637\n",
      "4300/10000 322.80672121047974\n",
      "4400/10000 330.21765518188477\n",
      "4500/10000 337.59280705451965\n",
      "4600/10000 346.00430488586426\n",
      "4700/10000 355.3043420314789\n",
      "4800/10000 363.3501899242401\n",
      "4900/10000 371.62449502944946\n",
      "5000/10000 379.4636387825012\n",
      "5100/10000 387.5284471511841\n",
      "5200/10000 395.10381507873535\n",
      "5300/10000 403.25609612464905\n",
      "5400/10000 411.1681921482086\n",
      "5500/10000 418.31722617149353\n",
      "5600/10000 425.941134929657\n",
      "5700/10000 433.3733751773834\n",
      "5800/10000 440.74462509155273\n",
      "5900/10000 448.6691360473633\n",
      "6000/10000 455.6246449947357\n",
      "6100/10000 463.74357414245605\n",
      "6200/10000 471.404305934906\n",
      "6300/10000 479.1176121234894\n",
      "6400/10000 486.48539304733276\n",
      "6500/10000 493.8457660675049\n",
      "6600/10000 501.09159302711487\n",
      "6700/10000 508.7599902153015\n",
      "6800/10000 516.7795631885529\n",
      "6900/10000 524.5675539970398\n",
      "7000/10000 531.743255853653\n",
      "7100/10000 539.1422400474548\n",
      "7200/10000 546.460401058197\n",
      "7300/10000 553.8635520935059\n",
      "7400/10000 560.8145880699158\n",
      "7500/10000 568.1481611728668\n",
      "7600/10000 575.5276508331299\n",
      "7700/10000 584.4862859249115\n",
      "7800/10000 592.3920121192932\n",
      "7900/10000 599.9930188655853\n",
      "8000/10000 607.2583010196686\n",
      "8100/10000 614.8477001190186\n",
      "8200/10000 622.373655796051\n",
      "8300/10000 629.8299000263214\n",
      "8400/10000 637.0746598243713\n",
      "8500/10000 644.2432169914246\n",
      "8600/10000 651.3160448074341\n",
      "8700/10000 658.4458289146423\n",
      "8800/10000 665.7993071079254\n",
      "8900/10000 672.9582438468933\n",
      "9000/10000 680.137580871582\n",
      "9100/10000 686.9984531402588\n",
      "9200/10000 694.1418969631195\n",
      "9300/10000 701.3232860565186\n",
      "9400/10000 708.443706035614\n",
      "9500/10000 715.5223398208618\n",
      "9600/10000 722.578540802002\n",
      "9700/10000 729.6068708896637\n",
      "9800/10000 736.7035658359528\n",
      "9900/10000 743.7795641422272\n",
      "750.9449071884155\n"
     ]
    }
   ],
   "source": [
    "cl_data = []\n",
    "start_time = time.time()\n",
    "for i, l in enumerate(test_data):\n",
    "    if (i>0 and i%500==0):\n",
    "        print(str(i)+'/'+str(len(test_data)), time.time()-start_time)\n",
    "    l = l.split(',', 1)\n",
    "    id_ = l[0]\n",
    "    tweet = l[1]\n",
    "    cl_data.append([id_, cl.classify(tweet)])\n",
    "print(time.time()-start_time)\n",
    "df = pd.DataFrame(cl_data)\n",
    "df.columns = ['Id', 'Prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Prediction'] = df['Prediction'].apply(lambda x : 1 if x == 'pos' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(model, test_data):\n",
    "    cl_data = []\n",
    "    for l in test_data:\n",
    "        l = l.split(',', 1)\n",
    "        id_ = l[0]\n",
    "        tweet = l[1]\n",
    "        cl_data.append([id_, model.classify(tweet)])\n",
    "    df = pd.DataFrame(cl_data)\n",
    "    df.columns = ['Id', 'Prediction']\n",
    "    df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      " <user> i .... this sucks ? ! ! haaappyyyyy dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      "\n",
      "['<user>', 'i', '....', 'this', 'sucks', '?', '!', '!', 'haaappyyyyy', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "['<user>', '...', 'negatively', 'suck', '<emphasis>', 'positively', '<<happy>>', 'dunno', 'justin', 'read', 'mention', '.', 'justin', 'god', 'know', ',', 'hope', 'follow', '<hashtag>', 'believe', '<number>']\n",
      "\n",
      "after:\n",
      " <user> ... negatively suck <emphasis> positively <<happy>> dunno justin read mention . justin god know , hope follow <hashtag> believe <number>\n"
     ]
    }
   ],
   "source": [
    "t = '<user> i .... this sucks ? ! ! haaappyyyyy dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\\n'\n",
    "print('original:\\n', t)\n",
    "t = t.replace('\\n', '')\n",
    "words = t.split(' ')\n",
    "print(words)\n",
    "\n",
    "words = removeNumbers(words)\n",
    "words = replaceHashtags(words)\n",
    "words = emphasizeWords(words)\n",
    "words = emphasizePunctiation(words)\n",
    "words = removeStopwords(words)\n",
    "words = normalizeWords(words)\n",
    "words = checkPositiveNegative(words)\n",
    "print(words)\n",
    "\n",
    "t = ' '.join(words)\n",
    "print('\\nafter:\\n', t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
