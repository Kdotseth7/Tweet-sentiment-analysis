{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from autocorrect import spell\n",
    "from hashtag_separator import infer_spaces\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "DATA_PATH = os.path.join('..', 'data')\n",
    "OUTPUT_PATH = os.path.join('..', 'output')\n",
    "\n",
    "# Training data set paths\n",
    "POS_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'pos_train.txt')\n",
    "NEG_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'neg_train.txt')\n",
    "POS_TRAIN_FULL_PATH = os.path.join(DATA_PATH, 'dataset', 'train_pos_full.txt')\n",
    "NEG_TRAIN_FULL_PATH = os.path.join(DATA_PATH, 'dataset', 'train_neg_full.txt')\n",
    "\n",
    "# Testing data set paths\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'dataset', 'test_data.txt')\n",
    "\n",
    "# Sentiment corpus\n",
    "POS_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'positive-words.txt')\n",
    "NEG_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'negative-words.txt')\n",
    "\n",
    "with open(POS_TRAIN_PATH, 'r') as f:\n",
    "    pos_data = f.read().splitlines()\n",
    "with open(NEG_TRAIN_PATH, 'r') as f:\n",
    "    neg_data = f.read().splitlines()\n",
    "with open(POS_TRAIN_FULL_PATH, 'r') as f:\n",
    "    pos_full_data = f.read().splitlines()\n",
    "with open(NEG_TRAIN_FULL_PATH, 'r') as f:\n",
    "    neg_full_data = f.read().splitlines()\n",
    "    \n",
    "with open(TEST_PATH, 'r') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "\n",
    "with open(POS_WORDS_PATH, 'r') as f:\n",
    "    pos_words = f.read().splitlines()\n",
    "with open(NEG_WORDS_PATH, 'r') as f:\n",
    "    neg_words = f.read().splitlines()\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    return list(filter(lambda x: x not in stopwords_eng, words))\n",
    "\n",
    "def is_number(word):\n",
    "    original_word = word\n",
    "    special_chars = ['.', ',', '/', '%', '-']\n",
    "    for char in special_chars:\n",
    "        word = word.replace(char, '')\n",
    "    if word.isdigit():\n",
    "        return ''\n",
    "    else:\n",
    "        return original_word\n",
    "    \n",
    "def remove_numbers(words):\n",
    "    return list(map(is_number, words))\n",
    "\n",
    "def replace_hashtags(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word and word[0]!='#':\n",
    "            new_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        for hash_word in infer_spaces(word[1:]).split(' '):\n",
    "            new_words.append(hash_word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "# Credit: https://stackoverflow.com/questions/10072744/remove-repeating-characters-from-words\n",
    "def emphasize(word):\n",
    "    if (word[:2]=='..'):\n",
    "        return '...'\n",
    "    \n",
    "    new_word = re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "    if len(new_word) != len(word):\n",
    "        return '<<' + spell(new_word) + '>>'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def emphasize_words(words):\n",
    "    return list(map(emphasize, words))\n",
    "\n",
    "def lemmatize(word):\n",
    "    try:\n",
    "        temp = lemmatizer.lemmatize(word).lower()\n",
    "        return temp\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_words(words):\n",
    "    return list(map(lemmatize, words))\n",
    "\n",
    "def infer_sentiment(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (word[:2]=='<<'):\n",
    "            check_word = word[2:-2]\n",
    "        else:\n",
    "            check_word = word\n",
    "        \n",
    "        if check_word in pos_words:\n",
    "            new_words += ['<<positive>>', word]\n",
    "        elif check_word in neg_words:\n",
    "            new_words += ['<<negative>>', word]\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "def emphasize_punctuation(words):\n",
    "    special_chars = ['!', '?']\n",
    "    i = 1\n",
    "    \n",
    "    while (i<len(words)):\n",
    "        word1 = words[i-1]\n",
    "        word2 = words[i]\n",
    "        if (word1 in special_chars and word2 in special_chars):\n",
    "            start = i-1\n",
    "            while (i+1<len(words) and words[i+1] in special_chars):\n",
    "                i += 1\n",
    "            words = words[:start] + ['<<emphasis>>'] + words[i+1:]\n",
    "            i = start\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return words\n",
    "\n",
    "def replace_emoticons(tweet):\n",
    "    emoticons = \\\n",
    "    [\n",
    "     ('<<positive>>',[ ':-)', ':)', '(:', '(-:', \\\n",
    "                       ':-D', ':D', 'X-D', 'XD', 'xD', \\\n",
    "                       '<3', ':\\*', ';-)', ';)', ';-D', ';D', '(;', '(-;', ] ),\\\n",
    "     ('<<negative>>', [':-(', ':(', '(:', '(-:', ':,(',\\\n",
    "                       ':\\'(', ':\"(', ':((', ] ),\\\n",
    "    ]\n",
    "\n",
    "    def replace_parenth(arr):\n",
    "        return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "    \n",
    "    def regex_join(arr):\n",
    "        return '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "    emoticons_regex = [ (repl, re.compile(regex_join(replace_parenth(regx))) ) \\\n",
    "            for (repl, regx) in emoticons ]\n",
    "    \n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        tweet = re.sub(regx, ' '+repl+' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def lower(tweet):\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tweet(t):\n",
    "    t = lower(t)\n",
    "    t = replace_emoticons(t)\n",
    "    words = t.split(' ')\n",
    "    words = remove_stopwords(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = replace_hashtags(words)\n",
    "    words = emphasize_words(words)\n",
    "    words = normalize_words(words)\n",
    "    words = infer_sentiment(words)\n",
    "    words = emphasize_punctuation(words)\n",
    "    tweet = ' '.join(words)\n",
    "    return tweet\n",
    "\n",
    "def parse_data(data):\n",
    "    parsed = []\n",
    "    start_time = time.time()\n",
    "    length = len(data)\n",
    "    \n",
    "    for i, t in enumerate(data):\n",
    "        if (i+1)%100000==0:\n",
    "            print(str(i+1)+'/'+str(len(data)), time.time()-start_time)\n",
    "        parsed.append(parse_tweet(t))\n",
    "    \n",
    "    print('Total time (s): ' + str(time.time()-start_time))\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 1250000\n",
      "Negatives: 1250000\n",
      "100000/1250000 167.78906798362732\n",
      "200000/1250000 310.06894183158875\n",
      "300000/1250000 452.5985391139984\n",
      "400000/1250000 585.5992059707642\n",
      "500000/1250000 725.152116060257\n",
      "600000/1250000 876.9700930118561\n",
      "700000/1250000 1025.842563867569\n",
      "800000/1250000 1166.373085975647\n",
      "900000/1250000 1304.1353511810303\n",
      "1000000/1250000 1447.7323589324951\n",
      "1100000/1250000 1592.6861300468445\n",
      "1200000/1250000 1732.7914249897003\n",
      "Total time (s): 1797.9385318756104\n",
      "100000/1250000 163.28157210350037\n",
      "200000/1250000 319.22611021995544\n",
      "300000/1250000 478.1678581237793\n",
      "400000/1250000 634.2569048404694\n",
      "500000/1250000 793.6341848373413\n",
      "600000/1250000 949.1574530601501\n",
      "700000/1250000 1098.9475939273834\n",
      "800000/1250000 1253.512937784195\n",
      "900000/1250000 1407.8736789226532\n",
      "1000000/1250000 1562.4150609970093\n",
      "1100000/1250000 1744.9414188861847\n",
      "1200000/1250000 1911.5277769565582\n",
      "Total time (s): 1990.4139518737793\n",
      "3788.3559160232544\n"
     ]
    }
   ],
   "source": [
    "print('Positives: ' + str(len(pos_full_data)))\n",
    "print('Negatives: ' + str(len(neg_full_data)))\n",
    "\n",
    "start = time.time()\n",
    "parsed_pos = parse_data(pos_full_data)\n",
    "parsed_neg = parse_data(neg_full_data)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARSED_TWEETS_PATH = os.path.join(OUTPUT_PATH, 'parsed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(parsed_pos)\n",
    "pos_df.columns = ['text']\n",
    "pos_df['label'] = 'pos'\n",
    "\n",
    "neg_df = pd.DataFrame(parsed_neg)\n",
    "neg_df.columns = ['text']\n",
    "neg_df['label'] = 'neg'\n",
    "\n",
    "df = pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(PARSED_TWEETS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PARSED_TWEETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_flat = df.as_matrix(['text']).astype(str).flatten()\n",
    "tweets_sentiment_flat = df.as_matrix(['label']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets_flat, tweets_sentiment_flat, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tweets = list(map(lambda x: 1 if x == 'pos' else -1, tweets_sentiment_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('nn', MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                         hidden_layer_sizes=(64,), random_state=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249.0632450580597\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn_pipe.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('svm', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C' : [0.1, 1, 5, 10],\n",
    "}\n",
    "\n",
    "CV_pipe = GridSearchCV(pipe, param_grid=param_grid, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "CV_pipe_nb = GridSearchCV(pipe_nb, param_grid={}, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543.0757880210876\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "CV_pipe_nb.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389.7558951377869\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "CV_pipe.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.79      0.81   1250000\n",
      "          1       0.80      0.84      0.82   1250000\n",
      "\n",
      "avg / total       0.81      0.81      0.81   2500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn_pipe.predict(tweets_flat)\n",
    "print(classification_report(y_tweets, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id                                              Tweet\n",
       "0  1  sea doo pro sea scooter ( sports with the port...\n",
       "1  2  <user> shucks well i work all week so now i ca...\n",
       "2  3            i cant stay away from bug thats my baby\n",
       "3  4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "4  5  whenever i fall asleep watching the tv , i alw..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_data = []\n",
    "\n",
    "for i, l in enumerate(test_data):\n",
    "    l = l.split(',', 1)\n",
    "    id_ = l[0]\n",
    "    tweet = l[1]\n",
    "    cl_data.append([id_, tweet])\n",
    "\n",
    "df = pd.DataFrame(cl_data)\n",
    "df.columns = ['Id', 'Tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time (s): 16.88913369178772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    sea doo pro sea scooter ( sport <<positive>> p...\n",
       "1    <user> shuck <<positive>> well <<positive>> wo...\n",
       "2           cant stay away <<negative>> bug thats baby\n",
       "3    <user> ma'am <<emphasis>> lol im <<positive>> ...\n",
       "4    whenever <<negative>> fall asleep watching tv ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tweet = parse_data(df.Tweet)\n",
    "df.Tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = nn_pipe.predict(df['Tweet'].as_matrix().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_PATH, 'submission.csv')\n",
    "\n",
    "res_df = pd.DataFrame({ 'Id': df['Id'].as_matrix().flatten(),\n",
    "                        'Prediction': y_pred})\n",
    "\n",
    "res_df = res_df.set_index('Id')\n",
    "res_df.to_csv(OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      " <user> i .... this sucks ? ! ! haaappyyyyy dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      "\n",
      "['<user>', 'i', '....', 'this', 'sucks', '?', '!', '!', 'haaappyyyyy', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "['<user>', '...', 'negatively', 'suck', '<emphasis>', 'positively', '<<happy>>', 'dunno', 'justin', 'read', 'mention', '.', 'justin', 'god', 'know', ',', 'hope', 'follow', '<hashtag>', 'believe', '<number>']\n",
      "\n",
      "after:\n",
      " <user> ... negatively suck <emphasis> positively <<happy>> dunno justin read mention . justin god know , hope follow <hashtag> believe <number>\n"
     ]
    }
   ],
   "source": [
    "t = '<user> i .... this sucks ? ! ! haaappyyyyy dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\\n'\n",
    "print('original:\\n', t)\n",
    "t = t.replace('\\n', '')\n",
    "words = t.split(' ')\n",
    "print(words)\n",
    "\n",
    "words = removeNumbers(words)\n",
    "words = replaceHashtags(words)\n",
    "words = emphasizeWords(words)\n",
    "words = emphasizePunctiation(words)\n",
    "words = removeStopwords(words)\n",
    "words = normalizeWords(words)\n",
    "words = checkPositiveNegative(words)\n",
    "print(words)\n",
    "\n",
    "t = ' '.join(words)\n",
    "print('\\nafter:\\n', t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
