{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from autocorrect import spell\n",
    "from hashtag_separator import infer_spaces\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "DATA_PATH = os.path.join('..', 'data')\n",
    "OUTPUT_PATH = os.path.join('..', 'output')\n",
    "\n",
    "# Training data set paths\n",
    "POS_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'pos_train.txt')\n",
    "NEG_TRAIN_PATH = os.path.join(DATA_PATH, 'dataset', 'neg_train.txt')\n",
    "POS_TRAIN_FULL_PATH = os.path.join(DATA_PATH, 'dataset', 'train_pos_full.txt')\n",
    "NEG_TRAIN_FULL_PATH = os.path.join(DATA_PATH, 'dataset', 'train_neg_full.txt')\n",
    "\n",
    "# Testing data set paths\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'dataset', 'test_data.txt')\n",
    "\n",
    "# Sentiment corpus\n",
    "POS_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'positive-words.txt')\n",
    "NEG_WORDS_PATH = os.path.join(DATA_PATH, 'sentiment', 'negative-words.txt')\n",
    "\n",
    "with open(POS_TRAIN_PATH, 'r') as f:\n",
    "    pos_data = f.read().splitlines()\n",
    "with open(NEG_TRAIN_PATH, 'r') as f:\n",
    "    neg_data = f.read().splitlines()\n",
    "with open(POS_TRAIN_FULL_PATH, 'r') as f:\n",
    "    pos_full_data = f.read().splitlines()\n",
    "with open(NEG_TRAIN_FULL_PATH, 'r') as f:\n",
    "    neg_full_data = f.read().splitlines()\n",
    "    \n",
    "with open(TEST_PATH, 'r') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "\n",
    "with open(POS_WORDS_PATH, 'r') as f:\n",
    "    pos_words = f.read().splitlines()\n",
    "with open(NEG_WORDS_PATH, 'r') as f:\n",
    "    neg_words = f.read().splitlines()\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    return list(filter(lambda x: x not in stopwords_eng, words))\n",
    "\n",
    "def is_number(word):\n",
    "    original_word = word\n",
    "    special_chars = ['.', ',', '/', '%', '-']\n",
    "    for char in special_chars:\n",
    "        word = word.replace(char, '')\n",
    "    if word.isdigit():\n",
    "        return ''\n",
    "    else:\n",
    "        return original_word\n",
    "    \n",
    "def remove_numbers(words):\n",
    "    return list(map(is_number, words))\n",
    "\n",
    "def replace_hashtags(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word and word[0]!='#':\n",
    "            new_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        for hash_word in infer_spaces(word[1:]).split(' '):\n",
    "            new_words.append(hash_word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "# Credit: https://stackoverflow.com/questions/10072744/remove-repeating-characters-from-words\n",
    "def emphasize(word):\n",
    "    if (word[:2]=='..'):\n",
    "        return '...'\n",
    "    \n",
    "    new_word = re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "    if len(new_word) != len(word):\n",
    "        return '<<' + spell(new_word) + '>>'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def emphasize_words(words):\n",
    "    return list(map(emphasize, words))\n",
    "\n",
    "def lemmatize(word):\n",
    "    try:\n",
    "        temp = lemmatizer.lemmatize(word).lower()\n",
    "        return temp\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_words(words):\n",
    "    return list(map(lemmatize, words))\n",
    "\n",
    "def infer_sentiment(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (word[:2]=='<<'):\n",
    "            check_word = word[2:-2]\n",
    "        else:\n",
    "            check_word = word\n",
    "        \n",
    "        if check_word in pos_words:\n",
    "            new_words += ['<<positive>>', word]\n",
    "        elif check_word in neg_words:\n",
    "            new_words += ['<<negative>>', word]\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "def emphasize_punctuation(words):\n",
    "    special_chars = ['!', '?']\n",
    "    i = 1\n",
    "    \n",
    "    while (i<len(words)):\n",
    "        word1 = words[i-1]\n",
    "        word2 = words[i]\n",
    "        if (word1 in special_chars and word2 in special_chars):\n",
    "            start = i-1\n",
    "            while (i+1<len(words) and words[i+1] in special_chars):\n",
    "                i += 1\n",
    "            words = words[:start] + ['<<emphasis>>'] + words[i+1:]\n",
    "            i = start\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return words\n",
    "\n",
    "def replace_emoticons(tweet):\n",
    "    emoticons = \\\n",
    "    [\n",
    "     ('<<positive>>',[ ':-)', ':)', '(:', '(-:', \\\n",
    "                       ':-D', ':D', 'X-D', 'XD', 'xD', \\\n",
    "                       '<3', ':\\*', ';-)', ';)', ';-D', ';D', '(;', '(-;', ] ),\\\n",
    "     ('<<negative>>', [':-(', ':(', '(:', '(-:', ':,(',\\\n",
    "                       ':\\'(', ':\"(', ':((', ] ),\\\n",
    "    ]\n",
    "\n",
    "    def replace_parenth(arr):\n",
    "        return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "    \n",
    "    def regex_join(arr):\n",
    "        return '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "    emoticons_regex = [ (repl, re.compile(regex_join(replace_parenth(regx))) ) \\\n",
    "            for (repl, regx) in emoticons ]\n",
    "    \n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        tweet = re.sub(regx, ' '+repl+' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def lower(tweet):\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tweet(t):\n",
    "    t = lower(t)\n",
    "    t = replace_emoticons(t)\n",
    "    words = t.split(' ')\n",
    "    words = remove_stopwords(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = replace_hashtags(words)\n",
    "    words = emphasize_words(words)\n",
    "    words = normalize_words(words)\n",
    "    words = infer_sentiment(words)\n",
    "    words = emphasize_punctuation(words)\n",
    "    tweet = ' '.join(words)\n",
    "    return tweet\n",
    "\n",
    "def parse_data(data):\n",
    "    parsed = []\n",
    "    start_time = time.time()\n",
    "    length = len(data)\n",
    "    \n",
    "    for i, t in enumerate(data):\n",
    "        if (i+1)%100000==0:\n",
    "            print(str(i+1)+'/'+str(len(data)), time.time()-start_time)\n",
    "        parsed.append(parse_tweet(t))\n",
    "    \n",
    "    print('Total time (s): ' + str(time.time()-start_time))\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positives: ' + str(len(pos_full_data)))\n",
    "print('Negatives: ' + str(len(neg_full_data)))\n",
    "\n",
    "start = time.time()\n",
    "parsed_pos = parse_data(pos_full_data)\n",
    "parsed_neg = parse_data(neg_full_data)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARSED_TWEETS_PATH = os.path.join(OUTPUT_PATH, 'parsed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8de673761ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_pos' is not defined"
     ]
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(parsed_pos)\n",
    "pos_df.columns = ['text']\n",
    "pos_df['label'] = 'pos'\n",
    "\n",
    "neg_df = pd.DataFrame(parsed_neg)\n",
    "neg_df.columns = ['text']\n",
    "neg_df['label'] = 'neg'\n",
    "\n",
    "df = pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(PARSED_TWEETS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PARSED_TWEETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_flat = df.as_matrix(['text']).astype(str).flatten()\n",
    "tweets_sentiment_flat = df.as_matrix(['label']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets_flat, tweets_sentiment_flat, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.split(' ') for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86731110"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.train([x.split(' ') for x in X_train], total_examples=len(X_train), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mujki/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('optimistic', 0.6302816867828369),\n",
       " ('humble', 0.5811644792556763),\n",
       " ('happiness', 0.5510971546173096),\n",
       " ('sane', 0.5478929877281189),\n",
       " ('confident', 0.540488064289093),\n",
       " ('strong', 0.5270357728004456),\n",
       " ('motivated', 0.5258761048316956),\n",
       " ('focused', 0.49630099534988403),\n",
       " ('improve', 0.4862516522407532),\n",
       " ('healthy', 0.4835071563720703)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('positive')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 43088\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.split(' ') for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mujki/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.split(' '), X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.split(' '), X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/mujki/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Conv2D, AveragePooling1D, MaxPooling1D, Dropout, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(y_train)\n",
    "temp[0] = temp[0].apply(lambda x: 1 if x == 'pos' else -1)\n",
    "temp_train = temp.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(y_test)\n",
    "temp[0] = temp[0].apply(lambda x: 1 if x == 'pos' else -1)\n",
    "temp_test = temp.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.expand_dims(train_vecs_w2v, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mujki/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 2, input_shape=(200, 1), padding=\"same\")`\n",
      "  \n",
      "/Users/mujki/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, 2, padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/mujki/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(16, 2, padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64, 2, border_mode='same', input_shape=(200, 1)))\n",
    "model.add(Conv1D(32, 2, border_mode='same'))\n",
    "model.add(Conv1D(16, 2, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(temp, temp_train, epochs=1, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.expand_dims(test_vecs_w2v, axis=2)\n",
    "y_pred = model.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tweets = list(map(lambda x: 1 if x == 'pos' else -1, tweets_sentiment_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('nn', MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                         hidden_layer_sizes=(64,), random_state=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "nn_pipe.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('svm', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C' : [0.1, 1, 5, 10],\n",
    "}\n",
    "\n",
    "CV_pipe = GridSearchCV(pipe, param_grid=param_grid, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    ('vec', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True)), \n",
    "    ('bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "CV_pipe_nb = GridSearchCV(pipe_nb, param_grid={}, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "CV_pipe_nb.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "CV_pipe.fit(tweets_flat, y_tweets)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = nn_pipe.predict(tweets_flat)\n",
    "print(classification_report(y_tweets, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_data = []\n",
    "\n",
    "for i, l in enumerate(test_data):\n",
    "    l = l.split(',', 1)\n",
    "    id_ = l[0]\n",
    "    tweet = l[1]\n",
    "    cl_data.append([id_, tweet])\n",
    "\n",
    "df = pd.DataFrame(cl_data)\n",
    "df.columns = ['Id', 'Tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Tweet = parse_data(df.Tweet)\n",
    "df.Tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = nn_pipe.predict(df['Tweet'].as_matrix().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_PATH, 'submission.csv')\n",
    "\n",
    "res_df = pd.DataFrame({ 'Id': df['Id'].as_matrix().flatten(),\n",
    "                        'Prediction': y_pred})\n",
    "\n",
    "res_df = res_df.set_index('Id')\n",
    "res_df.to_csv(OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '<user> i .... this sucks ? ! ! haaappyyyyy dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\\n'\n",
    "print('original:\\n', t)\n",
    "t = t.replace('\\n', '')\n",
    "words = t.split(' ')\n",
    "print(words)\n",
    "\n",
    "words = removeNumbers(words)\n",
    "words = replaceHashtags(words)\n",
    "words = emphasizeWords(words)\n",
    "words = emphasizePunctiation(words)\n",
    "words = removeStopwords(words)\n",
    "words = normalizeWords(words)\n",
    "words = checkPositiveNegative(words)\n",
    "print(words)\n",
    "\n",
    "t = ' '.join(words)\n",
    "print('\\nafter:\\n', t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
